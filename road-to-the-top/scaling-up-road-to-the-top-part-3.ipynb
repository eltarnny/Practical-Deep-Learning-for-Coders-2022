{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f17fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eadfab",
   "metadata": {},
   "source": [
    "## Memory and gradient accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c8e2512",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path.cwd()\n",
    "datapath = path/'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b2b79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_files = get_image_files(datapath/'test_images').sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c330ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "normal                      1764\n",
       "blast                       1738\n",
       "hispa                       1594\n",
       "dead_heart                  1442\n",
       "tungro                      1088\n",
       "brown_spot                   965\n",
       "downy_mildew                 620\n",
       "bacterial_leaf_blight        479\n",
       "bacterial_leaf_streak        380\n",
       "bacterial_panicle_blight     337\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the category with the least images for fast testing\n",
    "df = pd.read_csv(datapath/'train.csv')\n",
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2231c84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_path = datapath/'train_images'/'bacterial_panicle_blight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c61814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs=64//accum : we set the batch size based on 64 int div (//) accum(ulation)\n",
    "# But the smaller the batch size, the more volatility, and you need different learning rate\n",
    "# So we use Gradient Accumulation, where we add all the coefficients without zeroing them out\n",
    "# until the batch size reaches the one we want,\n",
    "# and then we subtract them using the lr and zeroing them out\n",
    "# see next cell example\n",
    "\n",
    "def train(arch, size, item=Resize(480, method='squish'), accum=1, finetune=True, epochs=12):\n",
    "    dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=item,\n",
    "        batch_tfms=aug_transforms(size=size, min_scale=0.75), bs=64//accum)\n",
    "    cbs = GradientAccumulation(64) if accum else [] # This is the call back that the lerner will use\n",
    "    learn = vision_learner(dls, arch, metrics=error_rate, cbs=cbs).to_fp16() # see cbs=cbs\n",
    "    if finetune:\n",
    "        learn.fine_tune(epochs, 0.01)\n",
    "        return learn.tta(dl=dls.test_dl(tst_files))\n",
    "    else:\n",
    "        learn.unfreeze()\n",
    "        learn.fit_one_cycle(epochs, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b294e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how Gradient Accumulation works (do not run here)\n",
    "# Instead of doing\n",
    "for x, y in dl:\n",
    "    calc_loss(coeffs, x, y).backward()\n",
    "    coeffs.data.sub_(coeffs.grad * lr)\n",
    "    coeffs.grad.zero_()\n",
    "\n",
    "# We do\n",
    "count = 0 # track count of items since last weight update\n",
    "for x, y in dl: # update count based on this minibatch size\n",
    "    count += len(x)\n",
    "    calc_loss(coeffs, x, y).backward()\n",
    "    if count > 64: # count is > accumulation target, do weight update\n",
    "        coeffs.data.sub_(coeffs.grad * lr)\n",
    "        coeffs.grad.zero_()\n",
    "        count = 0 # reset count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e656c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eltar\\anaconda3\\Lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name convnext_small_in22k to current convnext_small.fb_in22k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eltar\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\\aten\\src\\ATen\\native\\cudnn\\Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "# accum=1 means that the weights will be updated for the whole mini batch\n",
    "train('convnext_small_in22k', 128, epochs=1, accum=1, finetune=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0847bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cuda list_gpu_processes did not work on my PC, so I used nvidia-smi\n",
    "# Here we are trying to keep GPU memory usage under a specific limit by changing accum\n",
    "import subprocess\n",
    "\n",
    "def list_gpu_processes_with_nvidia_smi():\n",
    "    try:\n",
    "        # Run the nvidia-smi command to get full output\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi'], \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error executing nvidia-smi command:\")\n",
    "            print(result.stderr.decode('utf-8'))\n",
    "            return\n",
    "\n",
    "        # Decode the output\n",
    "        output = result.stdout.decode('utf-8')\n",
    "        \n",
    "        # Print the raw output to debug\n",
    "        print(output)\n",
    "        \n",
    "        # Parse the output manually to extract relevant details\n",
    "        lines = output.split('\\n')\n",
    "        process_section = False\n",
    "        processes = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if 'Processes:' in line:\n",
    "                process_section = True\n",
    "                continue\n",
    "            \n",
    "            if process_section and '===== ' in line:\n",
    "                break  # End of the processes section\n",
    "            \n",
    "            if process_section and line.strip():\n",
    "                processes.append(line.strip())\n",
    "        \n",
    "        if processes:\n",
    "            print(\"GPU Processes:\")\n",
    "            for process in processes:\n",
    "                print(process)\n",
    "        else:\n",
    "            print(\"No GPU processes found.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9da89800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: name 'subprocess' is not defined\n"
     ]
    }
   ],
   "source": [
    "def list_gpu_memory_usage():\n",
    "    try:\n",
    "        # Run the nvidia-smi command to get full output\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi'], \n",
    "            stdout=subprocess.PIPE, \n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        \n",
    "        # Check if the command was successful\n",
    "        if result.returncode != 0:\n",
    "            print(\"Error executing nvidia-smi command:\")\n",
    "            print(result.stderr.decode('utf-8'))\n",
    "            return\n",
    "\n",
    "        # Decode the output\n",
    "        output = result.stdout.decode('utf-8')\n",
    "        \n",
    "        # Print the raw output to debug\n",
    "        # print(output)\n",
    "        \n",
    "        # Parse the output manually to extract the memory usage section\n",
    "        lines = output.split('\\n')\n",
    "        memory_usage_section = False\n",
    "        memory_usage_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            if 'GPU' in line and 'Memory-Usage' in line:\n",
    "                memory_usage_section = True\n",
    "\n",
    "            if memory_usage_section:\n",
    "                memory_usage_lines.append(line)\n",
    "                if '+------------------------+' in line:\n",
    "                    break\n",
    "        \n",
    "        if memory_usage_lines:\n",
    "            for line in memory_usage_lines:\n",
    "                print(line)\n",
    "        else:\n",
    "            print(\"Memory usage information not found.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Execute the function\n",
    "list_gpu_memory_usage()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6b0e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_gpu():\n",
    "    # CUDA returned an error here, used the nvidia-smi version\n",
    "    # print(torch.cuda.list_gpu_processes())\n",
    "    list_gpu_memory_usage()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c0b2441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 51%   30C    P8              8W /  200W |     904MiB /  12282MiB |      4%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0a5f501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 52%   31C    P3             24W /  200W |    2763MiB /  12282MiB |     16%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "# accum=2 means that the weights will be updated for every 2 half sized mini batches\n",
    "train('convnext_small_in22k', 128, epochs=1, accum=2, finetune=False)\n",
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5afc86bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 53%   33C    P2             45W /  200W |    8689MiB /  12282MiB |     62%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "train('convnext_small_in22k', 128, epochs=1, accum=4, finetune=False)\n",
    "report_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7376e138",
   "metadata": {},
   "source": [
    "## Checking memory use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7bf9f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eltar\\anaconda3\\Lib\\site-packages\\timm\\models\\_factory.py:117: UserWarning: Mapping deprecated model name convnext_large_in22k to current convnext_large.fb_in22k.\n",
      "  model = create_fn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f442f3cd1724404aed2c9e46bf8b1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/919M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eltar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eltar\\.cache\\huggingface\\hub\\models--timm--convnext_large.fb_in22k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 54%   36C    P2             93W /  200W |   10648MiB /  12282MiB |     44%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "train('convnext_large_in22k', 224, epochs=1, accum=2, finetune=False)\n",
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adf6d642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 55%   37C    P2             59W /  200W |   11957MiB /  12282MiB |     75%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "train('convnext_large_in22k', (320,240), epochs=1, accum=2, finetune=False)\n",
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dbb9f38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9224762942254818851a0b090601da4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eltar\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eltar\\.cache\\huggingface\\hub\\models--timm--vit_large_patch16_224.augreg_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eltar\\anaconda3\\Lib\\site-packages\\timm\\models\\vision_transformer.py:92: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  x = F.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 55%   38C    P2             93W /  200W |   11905MiB /  12282MiB |     62%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "train('vit_large_patch16_224', 224, epochs=1, accum=2, finetune=False)\n",
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e62ad3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070      WDDM  |   00000000:07:00.0  On |                  N/A |\n",
      "| 52%   31C    P8              8W /  200W |    8715MiB /  12282MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n"
     ]
    }
   ],
   "source": [
    "report_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8cefbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could not run this model (Out of Memory)\n",
    "train('swin_large_patch4_window7_224', 224, epochs=1, accum=4, finetune=False)\n",
    "report_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a55b7f",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58260ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 640,480"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The idea here is to run an enseble of all the above models\n",
    "models = {\n",
    "    'convnext_large_in22k': {\n",
    "        (Resize(res), 224),\n",
    "        (Resize(res), (320,224)),\n",
    "    }, 'vit_large_patch16_224': {\n",
    "        (Resize(480, method='squish'), 224),\n",
    "        (Resize(res), 224),\n",
    "    }, 'swinv2_large_window12_192_22k': {\n",
    "        (Resize(480, method='squish'), 192),\n",
    "        (Resize(res), 192),\n",
    "    }, 'swin_large_patch4_window7_224': {\n",
    "        (Resize(480, method='squish'), 224),\n",
    "        (Resize(res), 224),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_path = datapath/'train_images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed5157",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_res = []\n",
    "\n",
    "for arch,details in models.items():\n",
    "    for item,size in details:\n",
    "        print('---',arch)\n",
    "        print(size)\n",
    "        print(item.name)\n",
    "        tta_res.append(train(arch, size, item=item, accum=2)) #, epochs=1))\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bd2d9b",
   "metadata": {},
   "source": [
    "## Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_pickle('tta_res.pkl', tta_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de40a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tta_prs = first(zip(*tta_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doubling the weights of the vit models (hack), as they were better\n",
    "tta_prs += tta_prs[2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d79d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of all the ensebled results\n",
    "avg_pr = torch.stack(tta_prs).mean(0)\n",
    "avg_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71eab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = ImageDataLoaders.from_folder(trn_path, valid_pct=0.2, item_tfms=Resize(480, method='squish'),\n",
    "    batch_tfms=aug_transforms(size=224, min_scale=0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bad75",
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = avg_pr.argmax(dim=1)\n",
    "vocab = np.array(dls.vocab)\n",
    "ss = pd.read_csv(datapath/'sample_submission.csv')\n",
    "ss['label'] = vocab[idxs]\n",
    "ss.to_csv('subm_03.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
